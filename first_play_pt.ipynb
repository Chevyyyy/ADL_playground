{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[8.9082e-39, 9.2755e-39, 9.6429e-39],\n",
      "          [8.4490e-39, 9.6429e-39, 9.6429e-39]],\n",
      "\n",
      "         [[1.0194e-38, 9.1837e-39, 8.4490e-39],\n",
      "          [1.0102e-38, 1.0561e-38, 9.0918e-39]]],\n",
      "\n",
      "\n",
      "        [[[1.0010e-38, 4.4083e-39, 5.2347e-39],\n",
      "          [4.1327e-39, 1.0653e-38, 9.9184e-39]],\n",
      "\n",
      "         [[9.0000e-39, 1.0561e-38, 1.0653e-38],\n",
      "          [4.1327e-39, 8.9082e-39, 9.8265e-39]]]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.empty(2,2,2,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5000, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([2.5,0.1])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9432, 0.6315],\n",
      "        [0.8883, 0.1257]])\n",
      "tensor([[0.4542, 0.1958],\n",
      "        [0.0523, 0.5016]])\n",
      "tensor([[1.3975, 0.8274],\n",
      "        [0.9406, 0.6273]])\n",
      "tensor([[1.3975, 0.8274],\n",
      "        [0.9406, 0.6273]])\n",
      "tensor([[1.3975, 0.8274],\n",
      "        [0.9406, 0.6273]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(2,2)\n",
    "y=torch.rand(2,2)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x+y)\n",
    "print(y.add(x))\n",
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0765,  3.2248],\n",
      "        [16.9958,  0.2505]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.div(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8711, 0.4478, 0.2505, 0.6187, 0.9338],\n",
      "        [0.7619, 0.2766, 0.6558, 0.0903, 0.8968],\n",
      "        [0.3981, 0.5785, 0.0466, 0.0243, 0.2119],\n",
      "        [0.1696, 0.0504, 0.0036, 0.4406, 0.4694],\n",
      "        [0.1850, 0.5524, 0.1006, 0.8994, 0.3343]])\n",
      "0.2765601873397827\n",
      "tensor([0.8711, 0.4478, 0.2505, 0.6187, 0.9338, 0.7619, 0.2766, 0.6558, 0.0903,\n",
      "        0.8968, 0.3981, 0.5785, 0.0466, 0.0243, 0.2119, 0.1696, 0.0504, 0.0036,\n",
      "        0.4406, 0.4694, 0.1850, 0.5524, 0.1006, 0.8994, 0.3343])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(5,5)\n",
    "print(x)\n",
    "print(x[1,1].item())\n",
    "# show all the element in one dimension\n",
    "print(x.view(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=torch.ones(5)\n",
    "\n",
    "print(a)\n",
    "b=a.numpy()\n",
    "print(b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\n",
    "# both changed due to point to the same memory\n",
    "# b=torch.from_numpy(a)\n",
    "a=a+1\n",
    "print(a)\n",
    "print(b)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"sss\")\n",
    "    device=torch.device('cuda')\n",
    "    x=torch.ones(5,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x=torch.ones(5,requires_grad=True)\n",
    "# true if grad needs to be calculated\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1855, -1.1642,  1.2852], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(3,requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0344, 1.3554, 1.6518], grad_fn=<MulBackward0>)\n",
      "tensor([-0.3711, -2.3284,  2.5704])\n",
      "tensor([-0.1855, -1.1642,  1.2852], requires_grad=True)\n",
      "tensor([0.0344, 1.3554, 1.6518], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y=x*x\n",
    "print(y)\n",
    "y.backward(torch.tensor([1.0,1.0,1.0]))\n",
    "print(x.grad)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1855, -1.1642,  1.2852], requires_grad=True)\n",
      "tensor([-0.1855, -1.1642,  1.2852], requires_grad=True)\n",
      "tensor([-0.1855, -1.1642,  1.2852])\n",
      "tensor([1.8145, 0.8358, 3.2852])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "\n",
    "\n",
    "x.requires_grad_(True)\n",
    "print(x)\n",
    "\n",
    "y=x.detach()\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y=x+2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights=torch.ones(4,requires_grad=True)\n",
    "print(weights)\n",
    "\n",
    "for epoch in range(1):\n",
    "    model_output=(weights*3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "\n",
    "    print(weights.grad)\n",
    "    # clear the grad (auto accumulate)\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backpropagation\n",
    "use chain rule!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(1.0)\n",
    "y=torch.tensor(2.0)\n",
    "\n",
    "w=torch.tensor(1.0,requires_grad=True)\n",
    "\n",
    "\n",
    "# forward path and compute the loss\n",
    "y_hat=w*x\n",
    "loss=(y_hat-y)**2\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# backward\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "#update weights\n",
    "#next epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "-120.0\n",
      "epoch 1: w = 1.200, loss= 30.00000000\n",
      "-48.0\n",
      "epoch 2: w = 1.680, loss= 4.79999924\n",
      "-19.200003\n",
      "epoch 3: w = 1.872, loss= 0.76800019\n",
      "-7.68\n",
      "epoch 4: w = 1.949, loss= 0.12288000\n",
      "-3.0720026\n",
      "epoch 5: w = 1.980, loss= 0.01966083\n",
      "-1.2287936\n",
      "epoch 6: w = 1.992, loss= 0.00314570\n",
      "-0.49152374\n",
      "epoch 7: w = 1.997, loss= 0.00050332\n",
      "-0.1966095\n",
      "epoch 8: w = 1.999, loss= 0.00008053\n",
      "-0.07864165\n",
      "epoch 9: w = 1.999, loss= 0.00001288\n",
      "-0.031455517\n",
      "epoch 10: w = 2.000, loss= 0.00000206\n",
      "9.998951458930968\n"
     ]
    }
   ],
   "source": [
    "X=np.array([1,2,3,4],dtype=np.float32)\n",
    "Y=np.array([2,4,6,8],dtype=np.float32)\n",
    "\n",
    "\n",
    "w=0.0\n",
    "\n",
    "\n",
    "#model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#loss\n",
    "def loss(y,y_predicted):\n",
    "    return ((y-y_predicted)**2).mean()\n",
    "\n",
    "#gradient\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x,y_predicted-y).mean()\n",
    "\n",
    "print(forward(5))\n",
    "\n",
    "#training\n",
    "lr=0.01\n",
    "n_iters=10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred=forward(X)\n",
    "\n",
    "    l=loss(Y,y_pred)\n",
    "\n",
    "\n",
    "    dw=gradient(X,Y,y_pred)\n",
    "    print(dw)\n",
    "\n",
    "    w=w-lr*dw\n",
    "\n",
    "    if(epoch % 1)==0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss= {l:.8f}')\n",
    "print(forward(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do it with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<MulBackward0>)\n",
      "tensor(-30.)\n",
      "epoch 1: w = 0.300, loss= 30.00000000\n",
      "tensor(-25.5000)\n",
      "epoch 2: w = 0.555, loss= 21.67499924\n",
      "tensor(-21.6750)\n",
      "epoch 3: w = 0.772, loss= 15.66018772\n",
      "tensor(-18.4238)\n",
      "epoch 4: w = 0.956, loss= 11.31448650\n",
      "tensor(-15.6602)\n",
      "epoch 5: w = 1.113, loss= 8.17471695\n",
      "tensor(-13.3112)\n",
      "epoch 6: w = 1.246, loss= 5.90623236\n",
      "tensor(-11.3145)\n",
      "epoch 7: w = 1.359, loss= 4.26725292\n",
      "tensor(-9.6173)\n",
      "epoch 8: w = 1.455, loss= 3.08308983\n",
      "tensor(-8.1747)\n",
      "epoch 9: w = 1.537, loss= 2.22753215\n",
      "tensor(-6.9485)\n",
      "epoch 10: w = 1.606, loss= 1.60939169\n",
      "tensor(-5.9062)\n",
      "epoch 11: w = 1.665, loss= 1.16278565\n",
      "tensor(-5.0203)\n",
      "epoch 12: w = 1.716, loss= 0.84011245\n",
      "tensor(-4.2673)\n",
      "epoch 13: w = 1.758, loss= 0.60698116\n",
      "tensor(-3.6272)\n",
      "epoch 14: w = 1.794, loss= 0.43854395\n",
      "tensor(-3.0831)\n",
      "epoch 15: w = 1.825, loss= 0.31684780\n",
      "tensor(-2.6206)\n",
      "epoch 16: w = 1.851, loss= 0.22892261\n",
      "tensor(-2.2275)\n",
      "epoch 17: w = 1.874, loss= 0.16539653\n",
      "tensor(-1.8934)\n",
      "epoch 18: w = 1.893, loss= 0.11949898\n",
      "tensor(-1.6094)\n",
      "epoch 19: w = 1.909, loss= 0.08633806\n",
      "tensor(-1.3680)\n",
      "epoch 20: w = 1.922, loss= 0.06237914\n",
      "tensor(-1.1628)\n",
      "epoch 21: w = 1.934, loss= 0.04506890\n",
      "tensor(-0.9884)\n",
      "epoch 22: w = 1.944, loss= 0.03256231\n",
      "tensor(-0.8401)\n",
      "epoch 23: w = 1.952, loss= 0.02352631\n",
      "tensor(-0.7141)\n",
      "epoch 24: w = 1.960, loss= 0.01699772\n",
      "tensor(-0.6070)\n",
      "epoch 25: w = 1.966, loss= 0.01228084\n",
      "tensor(-0.5159)\n",
      "epoch 26: w = 1.971, loss= 0.00887291\n",
      "tensor(-0.4385)\n",
      "epoch 27: w = 1.975, loss= 0.00641066\n",
      "tensor(-0.3728)\n",
      "epoch 28: w = 1.979, loss= 0.00463169\n",
      "tensor(-0.3168)\n",
      "epoch 29: w = 1.982, loss= 0.00334642\n",
      "tensor(-0.2693)\n",
      "epoch 30: w = 1.985, loss= 0.00241778\n",
      "tensor(-0.2289)\n",
      "epoch 31: w = 1.987, loss= 0.00174685\n",
      "tensor(-0.1946)\n",
      "epoch 32: w = 1.989, loss= 0.00126211\n",
      "tensor(-0.1654)\n",
      "epoch 33: w = 1.991, loss= 0.00091188\n",
      "tensor(-0.1406)\n",
      "epoch 34: w = 1.992, loss= 0.00065882\n",
      "tensor(-0.1195)\n",
      "epoch 35: w = 1.993, loss= 0.00047601\n",
      "tensor(-0.1016)\n",
      "epoch 36: w = 1.994, loss= 0.00034392\n",
      "tensor(-0.0863)\n",
      "epoch 37: w = 1.995, loss= 0.00024848\n",
      "tensor(-0.0734)\n",
      "epoch 38: w = 1.996, loss= 0.00017952\n",
      "tensor(-0.0624)\n",
      "epoch 39: w = 1.996, loss= 0.00012971\n",
      "tensor(-0.0530)\n",
      "epoch 40: w = 1.997, loss= 0.00009371\n",
      "tensor(-0.0451)\n",
      "epoch 41: w = 1.997, loss= 0.00006770\n",
      "tensor(-0.0383)\n",
      "epoch 42: w = 1.998, loss= 0.00004891\n",
      "tensor(-0.0326)\n",
      "epoch 43: w = 1.998, loss= 0.00003534\n",
      "tensor(-0.0277)\n",
      "epoch 44: w = 1.998, loss= 0.00002553\n",
      "tensor(-0.0235)\n",
      "epoch 45: w = 1.999, loss= 0.00001845\n",
      "tensor(-0.0200)\n",
      "epoch 46: w = 1.999, loss= 0.00001333\n",
      "tensor(-0.0170)\n",
      "epoch 47: w = 1.999, loss= 0.00000963\n",
      "tensor(-0.0144)\n",
      "epoch 48: w = 1.999, loss= 0.00000696\n",
      "tensor(-0.0123)\n",
      "epoch 49: w = 1.999, loss= 0.00000503\n",
      "tensor(-0.0104)\n",
      "epoch 50: w = 1.999, loss= 0.00000363\n",
      "tensor(-0.0089)\n",
      "epoch 51: w = 1.999, loss= 0.00000262\n",
      "tensor(-0.0075)\n",
      "epoch 52: w = 2.000, loss= 0.00000190\n",
      "tensor(-0.0064)\n",
      "epoch 53: w = 2.000, loss= 0.00000137\n",
      "tensor(-0.0054)\n",
      "epoch 54: w = 2.000, loss= 0.00000099\n",
      "tensor(-0.0046)\n",
      "epoch 55: w = 2.000, loss= 0.00000071\n",
      "tensor(-0.0039)\n",
      "epoch 56: w = 2.000, loss= 0.00000052\n",
      "tensor(-0.0033)\n",
      "epoch 57: w = 2.000, loss= 0.00000037\n",
      "tensor(-0.0028)\n",
      "epoch 58: w = 2.000, loss= 0.00000027\n",
      "tensor(-0.0024)\n",
      "epoch 59: w = 2.000, loss= 0.00000019\n",
      "tensor(-0.0021)\n",
      "epoch 60: w = 2.000, loss= 0.00000014\n",
      "tensor(-0.0017)\n",
      "epoch 61: w = 2.000, loss= 0.00000010\n",
      "tensor(-0.0015)\n",
      "epoch 62: w = 2.000, loss= 0.00000007\n",
      "tensor(-0.0013)\n",
      "epoch 63: w = 2.000, loss= 0.00000005\n",
      "tensor(-0.0011)\n",
      "epoch 64: w = 2.000, loss= 0.00000004\n",
      "tensor(-0.0009)\n",
      "epoch 65: w = 2.000, loss= 0.00000003\n",
      "tensor(-0.0008)\n",
      "epoch 66: w = 2.000, loss= 0.00000002\n",
      "tensor(-0.0007)\n",
      "epoch 67: w = 2.000, loss= 0.00000001\n",
      "tensor(-0.0006)\n",
      "epoch 68: w = 2.000, loss= 0.00000001\n",
      "tensor(-0.0005)\n",
      "epoch 69: w = 2.000, loss= 0.00000001\n",
      "tensor(-0.0004)\n",
      "epoch 70: w = 2.000, loss= 0.00000001\n",
      "tensor(-0.0003)\n",
      "epoch 71: w = 2.000, loss= 0.00000000\n",
      "tensor(-0.0003)\n",
      "epoch 72: w = 2.000, loss= 0.00000000\n",
      "tensor(-0.0002)\n",
      "epoch 73: w = 2.000, loss= 0.00000000\n",
      "tensor(-0.0002)\n",
      "epoch 74: w = 2.000, loss= 0.00000000\n",
      "tensor(-0.0002)\n",
      "epoch 75: w = 2.000, loss= 0.00000000\n",
      "tensor(-0.0002)\n",
      "epoch 76: w = 2.000, loss= 0.00000000\n",
      "tensor(-0.0001)\n",
      "epoch 77: w = 2.000, loss= 0.00000000\n",
      "tensor(-0.0001)\n",
      "epoch 78: w = 2.000, loss= 0.00000000\n",
      "tensor(-9.2983e-05)\n",
      "epoch 79: w = 2.000, loss= 0.00000000\n",
      "tensor(-7.8678e-05)\n",
      "epoch 80: w = 2.000, loss= 0.00000000\n",
      "tensor(-6.6340e-05)\n",
      "epoch 81: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.5254e-05)\n",
      "epoch 82: w = 2.000, loss= 0.00000000\n",
      "tensor(-4.6849e-05)\n",
      "epoch 83: w = 2.000, loss= 0.00000000\n",
      "tensor(-3.8981e-05)\n",
      "epoch 84: w = 2.000, loss= 0.00000000\n",
      "tensor(-3.3796e-05)\n",
      "epoch 85: w = 2.000, loss= 0.00000000\n",
      "tensor(-2.8610e-05)\n",
      "epoch 86: w = 2.000, loss= 0.00000000\n",
      "tensor(-2.4676e-05)\n",
      "epoch 87: w = 2.000, loss= 0.00000000\n",
      "tensor(-2.1458e-05)\n",
      "epoch 88: w = 2.000, loss= 0.00000000\n",
      "tensor(-1.8239e-05)\n",
      "epoch 89: w = 2.000, loss= 0.00000000\n",
      "tensor(-1.4305e-05)\n",
      "epoch 90: w = 2.000, loss= 0.00000000\n",
      "tensor(-1.2338e-05)\n",
      "epoch 91: w = 2.000, loss= 0.00000000\n",
      "tensor(-1.0371e-05)\n",
      "epoch 92: w = 2.000, loss= 0.00000000\n",
      "tensor(-9.1195e-06)\n",
      "epoch 93: w = 2.000, loss= 0.00000000\n",
      "tensor(-7.1526e-06)\n",
      "epoch 94: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 95: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 96: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 97: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 98: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 99: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 100: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 101: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 102: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 103: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 104: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 105: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 106: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 107: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 108: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 109: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 110: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 111: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 112: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 113: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 114: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 115: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 116: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 117: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 118: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 119: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 120: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 121: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 122: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 123: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 124: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 125: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 126: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 127: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 128: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 129: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 130: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 131: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 132: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 133: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 134: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 135: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 136: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 137: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 138: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 139: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 140: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 141: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 142: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 143: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 144: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 145: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 146: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 147: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 148: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 149: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 150: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 151: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 152: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 153: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 154: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 155: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 156: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 157: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 158: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 159: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 160: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 161: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 162: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 163: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 164: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 165: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 166: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 167: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 168: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 169: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 170: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 171: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 172: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 173: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 174: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 175: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 176: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 177: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 178: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 179: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 180: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 181: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 182: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 183: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 184: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 185: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 186: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 187: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 188: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 189: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 190: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 191: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 192: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 193: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 194: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 195: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 196: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 197: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 198: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 199: w = 2.000, loss= 0.00000000\n",
      "tensor(-5.1856e-06)\n",
      "epoch 200: w = 2.000, loss= 0.00000000\n",
      "tensor(10.0000, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X=torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "Y=torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "\n",
    "\n",
    "w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "#model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#loss\n",
    "def loss(y,y_predicted):\n",
    "    return ((y-y_predicted)**2).mean()\n",
    "\n",
    "\n",
    "print(forward(5))\n",
    "\n",
    "#training\n",
    "lr=0.01\n",
    "n_iters=200\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred=forward(X)\n",
    "\n",
    "    l=loss(Y,y_pred)\n",
    "\n",
    "\n",
    "    l.backward() #dl/dw\n",
    "    dw=w.grad\n",
    "    print(dw)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w-=lr*dw\n",
    "\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if(epoch % 1)==0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss= {l:.8f}')\n",
    "print(forward(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30., grad_fn=<MeanBackward0>)\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Academic\\miniconda\\envs\\comp0197-pt\\lib\\site-packages\\torch\\_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  C:\\b\\abs_f0dma8qm3d\\croot\\pytorch_1669187301762\\work\\build\\aten\\src\\ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "X=torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "Y=torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "\n",
    "\n",
    "w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n",
    "s=Y-X*w\n",
    "s.requires_grad_(True)\n",
    "\n",
    "loss=((Y-X*w)**2).mean()\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "print(s.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizer and autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<MulBackward0>)\n",
      "epoch 1: w = 0.300, loss= 30.00000000\n",
      "epoch 11: w = 1.665, loss= 1.16278565\n",
      "epoch 21: w = 1.934, loss= 0.04506890\n",
      "epoch 31: w = 1.987, loss= 0.00174685\n",
      "epoch 41: w = 1.997, loss= 0.00006770\n",
      "epoch 51: w = 1.999, loss= 0.00000262\n",
      "epoch 61: w = 2.000, loss= 0.00000010\n",
      "epoch 71: w = 2.000, loss= 0.00000000\n",
      "epoch 81: w = 2.000, loss= 0.00000000\n",
      "epoch 91: w = 2.000, loss= 0.00000000\n",
      "tensor(10.0000, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "X=torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "Y=torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "\n",
    "\n",
    "w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "#model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "\n",
    "print(forward(5))\n",
    "\n",
    "#training\n",
    "lr=0.01\n",
    "n_iters=100\n",
    "loss=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD([w],lr=lr)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #predication\n",
    "    y_pred=forward(X)\n",
    "\n",
    "\n",
    "    # loss\n",
    "    l=loss(Y,y_pred)\n",
    "\n",
    "\n",
    "    l.backward() #dl/dw\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "    if(epoch % 10)==0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss= {l:.8f}')\n",
    "print(forward(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch model (nn.linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "epoch 1: w = 3.552, loss= 61.40268326\n",
      "epoch 11: w = 1.908, loss= 0.04389156\n",
      "epoch 21: w = 1.909, loss= 0.01278361\n",
      "epoch 31: w = 1.933, loss= 0.00695670\n",
      "epoch 41: w = 1.950, loss= 0.00378777\n",
      "epoch 51: w = 1.963, loss= 0.00206235\n",
      "epoch 61: w = 1.973, loss= 0.00112291\n",
      "epoch 71: w = 1.980, loss= 0.00061140\n",
      "epoch 81: w = 1.985, loss= 0.00033289\n",
      "epoch 91: w = 1.989, loss= 0.00018125\n",
      "tensor([[9.9587]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X=torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "Y=torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
    "\n",
    "\n",
    "X_test=torch.tensor([5],dtype=torch.float32)\n",
    "\n",
    "n_sample,n_feature =X.shape\n",
    "print(n_sample,n_feature)\n",
    "\n",
    "\n",
    "input_size=n_feature\n",
    "output_size=n_feature\n",
    "# model=nn.Linear(input_size,output_size)\n",
    "\n",
    "#### customise your own model\n",
    "class linearRegression(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim) -> None:\n",
    "        super(linearRegression,self).__init__()\n",
    "\n",
    "        self.lin=nn.Linear(input_dim,output_dim)\n",
    "    \n",
    "    def forward (self,x):\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model=linearRegression(input_size,output_size)\n",
    "\n",
    "w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "#training\n",
    "lr=0.1\n",
    "n_iters=100\n",
    "loss=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #predication\n",
    "    y_pred=model(X)\n",
    "\n",
    "\n",
    "    # loss\n",
    "    l=loss(Y,y_pred)\n",
    "\n",
    "\n",
    "    l.backward() #dl/dw\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "    if(epoch % 10)==0:\n",
    "        [w,b]=model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss= {l:.8f}')\n",
    "\n",
    "print(forward(5))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear regression (all together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Academic\\miniconda\\envs\\comp0197-pt\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10,loss=3954.7412\n",
      "epoch:20,loss=2895.1572\n",
      "epoch:30,loss=2127.1482\n",
      "epoch:40,loss=1569.9578\n",
      "epoch:50,loss=1165.3660\n",
      "epoch:60,loss=871.3459\n",
      "epoch:70,loss=657.5226\n",
      "epoch:80,loss=501.9167\n",
      "epoch:90,loss=388.6070\n",
      "epoch:100,loss=306.0498\n",
      "epoch:110,loss=245.8682\n",
      "epoch:120,loss=201.9761\n",
      "epoch:130,loss=169.9507\n",
      "epoch:140,loss=146.5746\n",
      "epoch:150,loss=129.5056\n",
      "epoch:160,loss=117.0378\n",
      "epoch:170,loss=107.9282\n",
      "epoch:180,loss=101.2704\n",
      "epoch:190,loss=96.4034\n",
      "epoch:200,loss=92.8446\n",
      "epoch:210,loss=90.2420\n",
      "epoch:220,loss=88.3381\n",
      "epoch:230,loss=86.9453\n",
      "epoch:240,loss=85.9261\n",
      "epoch:250,loss=85.1802\n",
      "epoch:260,loss=84.6343\n",
      "epoch:270,loss=84.2347\n",
      "epoch:280,loss=83.9421\n",
      "epoch:290,loss=83.7279\n",
      "epoch:300,loss=83.5711\n",
      "epoch:310,loss=83.4562\n",
      "epoch:320,loss=83.3721\n",
      "epoch:330,loss=83.3105\n",
      "epoch:340,loss=83.2654\n",
      "epoch:350,loss=83.2324\n",
      "epoch:360,loss=83.2082\n",
      "epoch:370,loss=83.1905\n",
      "epoch:380,loss=83.1775\n",
      "epoch:390,loss=83.1679\n",
      "epoch:400,loss=83.1610\n",
      "epoch:410,loss=83.1559\n",
      "epoch:420,loss=83.1521\n",
      "epoch:430,loss=83.1494\n",
      "epoch:440,loss=83.1474\n",
      "epoch:450,loss=83.1459\n",
      "epoch:460,loss=83.1448\n",
      "epoch:470,loss=83.1441\n",
      "epoch:480,loss=83.1435\n",
      "epoch:490,loss=83.1431\n",
      "epoch:500,loss=83.1427\n",
      "epoch:510,loss=83.1425\n",
      "epoch:520,loss=83.1423\n",
      "epoch:530,loss=83.1422\n",
      "epoch:540,loss=83.1421\n",
      "epoch:550,loss=83.1421\n",
      "epoch:560,loss=83.1420\n",
      "epoch:570,loss=83.1420\n",
      "epoch:580,loss=83.1420\n",
      "epoch:590,loss=83.1419\n",
      "epoch:600,loss=83.1419\n",
      "epoch:610,loss=83.1419\n",
      "epoch:620,loss=83.1419\n",
      "epoch:630,loss=83.1419\n",
      "epoch:640,loss=83.1419\n",
      "epoch:650,loss=83.1419\n",
      "epoch:660,loss=83.1419\n",
      "epoch:670,loss=83.1419\n",
      "epoch:680,loss=83.1419\n",
      "epoch:690,loss=83.1419\n",
      "epoch:700,loss=83.1419\n",
      "epoch:710,loss=83.1419\n",
      "epoch:720,loss=83.1419\n",
      "epoch:730,loss=83.1419\n",
      "epoch:740,loss=83.1419\n",
      "epoch:750,loss=83.1419\n",
      "epoch:760,loss=83.1419\n",
      "epoch:770,loss=83.1419\n",
      "epoch:780,loss=83.1419\n",
      "epoch:790,loss=83.1419\n",
      "epoch:800,loss=83.1419\n",
      "epoch:810,loss=83.1419\n",
      "epoch:820,loss=83.1419\n",
      "epoch:830,loss=83.1419\n",
      "epoch:840,loss=83.1419\n",
      "epoch:850,loss=83.1419\n",
      "epoch:860,loss=83.1419\n",
      "epoch:870,loss=83.1419\n",
      "epoch:880,loss=83.1419\n",
      "epoch:890,loss=83.1419\n",
      "epoch:900,loss=83.1419\n",
      "epoch:910,loss=83.1419\n",
      "epoch:920,loss=83.1419\n",
      "epoch:930,loss=83.1419\n",
      "epoch:940,loss=83.1419\n",
      "epoch:950,loss=83.1419\n",
      "epoch:960,loss=83.1419\n",
      "epoch:970,loss=83.1419\n",
      "epoch:980,loss=83.1419\n",
      "epoch:990,loss=83.1419\n",
      "epoch:1000,loss=83.1419\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 0) prepare data\n",
    "\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100,n_features=1,noise=10,random_state=1)\n",
    "X=torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y=torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y=y.view(y.shape[0],1)\n",
    "\n",
    "n_samples, n_feature= X.shape\n",
    "\n",
    "\n",
    "\n",
    "#1) model\n",
    "model=nn.Linear(n_feature,1)\n",
    "\n",
    "\n",
    "#2) loss and optimizer\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "#3) training loop\n",
    "num_epoch=1000\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    # forward and loss\n",
    "    y_pred=model(X)\n",
    "    loss=criterion(y_pred,y)\n",
    "\n",
    "    # back pass\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    if (epoch+1)%10==0:\n",
    "        print(f'epoch:{epoch+1},loss={loss.item():.4f}')\n",
    "\n",
    "\n",
    "prediction =model(X).detach().numpy()\n",
    "\n",
    "plt.plot(X_numpy,y_numpy,\"ro\")\n",
    "plt.plot(X_numpy,prediction,\"b\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([114, 30])\n",
      "torch.Size([114, 1])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0) prepare data\n",
    "bc=datasets.load_breast_cancer()\n",
    "X,y=bc.data,bc.target\n",
    "\n",
    "n_samples,n_features=X.shape\n",
    "\n",
    "X_train,X_test,y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=1234)\n",
    "\n",
    "#scale \n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n",
    "\n",
    "X_train=torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test=torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train=torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test=torch.from_numpy(y_test.astype(np.float32))\n",
    "y_train=y_train.view(y_train.shape[0],1)\n",
    "y_test=y_test.view(y_test.shape[0],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.70\n",
      "epoch 1, loss: 0.64\n",
      "epoch 2, loss: 0.57\n",
      "epoch 3, loss: 0.51\n",
      "epoch 4, loss: 0.44\n",
      "epoch 5, loss: 0.38\n",
      "epoch 6, loss: 0.32\n",
      "epoch 7, loss: 0.26\n",
      "epoch 8, loss: 0.22\n",
      "epoch 9, loss: 0.17\n",
      "epoch 10, loss: 0.14\n",
      "epoch 11, loss: 0.11\n",
      "epoch 12, loss: 0.08\n",
      "epoch 13, loss: 0.07\n",
      "epoch 14, loss: 0.06\n",
      "epoch 15, loss: 0.05\n",
      "epoch 16, loss: 0.04\n",
      "epoch 17, loss: 0.04\n",
      "epoch 18, loss: 0.03\n",
      "epoch 19, loss: 0.03\n",
      "epoch 20, loss: 0.03\n",
      "epoch 21, loss: 0.03\n",
      "epoch 22, loss: 0.03\n",
      "epoch 23, loss: 0.02\n",
      "epoch 24, loss: 0.02\n",
      "epoch 25, loss: 0.02\n",
      "epoch 26, loss: 0.02\n",
      "epoch 27, loss: 0.02\n",
      "epoch 28, loss: 0.02\n",
      "epoch 29, loss: 0.02\n",
      "epoch 30, loss: 0.02\n",
      "epoch 31, loss: 0.02\n",
      "epoch 32, loss: 0.02\n",
      "epoch 33, loss: 0.02\n",
      "epoch 34, loss: 0.02\n",
      "epoch 35, loss: 0.02\n",
      "epoch 36, loss: 0.02\n",
      "epoch 37, loss: 0.02\n",
      "epoch 38, loss: 0.01\n",
      "epoch 39, loss: 0.01\n",
      "epoch 40, loss: 0.01\n",
      "epoch 41, loss: 0.01\n",
      "epoch 42, loss: 0.01\n",
      "epoch 43, loss: 0.01\n",
      "epoch 44, loss: 0.01\n",
      "epoch 45, loss: 0.01\n",
      "epoch 46, loss: 0.01\n",
      "epoch 47, loss: 0.01\n",
      "epoch 48, loss: 0.01\n",
      "epoch 49, loss: 0.01\n",
      "epoch 50, loss: 0.01\n",
      "epoch 51, loss: 0.01\n",
      "epoch 52, loss: 0.01\n",
      "epoch 53, loss: 0.01\n",
      "epoch 54, loss: 0.01\n",
      "epoch 55, loss: 0.01\n",
      "epoch 56, loss: 0.01\n",
      "epoch 57, loss: 0.01\n",
      "epoch 58, loss: 0.01\n",
      "epoch 59, loss: 0.01\n",
      "epoch 60, loss: 0.01\n",
      "epoch 61, loss: 0.01\n",
      "epoch 62, loss: 0.01\n",
      "epoch 63, loss: 0.01\n",
      "epoch 64, loss: 0.01\n",
      "epoch 65, loss: 0.01\n",
      "epoch 66, loss: 0.01\n",
      "epoch 67, loss: 0.01\n",
      "epoch 68, loss: 0.01\n",
      "epoch 69, loss: 0.01\n",
      "epoch 70, loss: 0.01\n",
      "epoch 71, loss: 0.01\n",
      "epoch 72, loss: 0.01\n",
      "epoch 73, loss: 0.01\n",
      "epoch 74, loss: 0.01\n",
      "epoch 75, loss: 0.01\n",
      "epoch 76, loss: 0.01\n",
      "epoch 77, loss: 0.01\n",
      "epoch 78, loss: 0.01\n",
      "epoch 79, loss: 0.01\n",
      "epoch 80, loss: 0.01\n",
      "epoch 81, loss: 0.01\n",
      "epoch 82, loss: 0.01\n",
      "epoch 83, loss: 0.01\n",
      "epoch 84, loss: 0.01\n",
      "epoch 85, loss: 0.01\n",
      "epoch 86, loss: 0.01\n",
      "epoch 87, loss: 0.01\n",
      "epoch 88, loss: 0.01\n",
      "epoch 89, loss: 0.01\n",
      "epoch 90, loss: 0.01\n",
      "epoch 91, loss: 0.01\n",
      "epoch 92, loss: 0.01\n",
      "epoch 93, loss: 0.01\n",
      "epoch 94, loss: 0.01\n",
      "epoch 95, loss: 0.01\n",
      "epoch 96, loss: 0.01\n",
      "epoch 97, loss: 0.01\n",
      "epoch 98, loss: 0.01\n",
      "epoch 99, loss: 0.01\n",
      "epoch 100, loss: 0.01\n",
      "epoch 101, loss: 0.00\n",
      "epoch 102, loss: 0.00\n",
      "epoch 103, loss: 0.00\n",
      "epoch 104, loss: 0.00\n",
      "epoch 105, loss: 0.00\n",
      "epoch 106, loss: 0.00\n",
      "epoch 107, loss: 0.00\n",
      "epoch 108, loss: 0.00\n",
      "epoch 109, loss: 0.00\n",
      "epoch 110, loss: 0.00\n",
      "epoch 111, loss: 0.00\n",
      "epoch 112, loss: 0.00\n",
      "epoch 113, loss: 0.00\n",
      "epoch 114, loss: 0.00\n",
      "epoch 115, loss: 0.00\n",
      "epoch 116, loss: 0.00\n",
      "epoch 117, loss: 0.00\n",
      "epoch 118, loss: 0.00\n",
      "epoch 119, loss: 0.00\n",
      "epoch 120, loss: 0.00\n",
      "epoch 121, loss: 0.00\n",
      "epoch 122, loss: 0.00\n",
      "epoch 123, loss: 0.00\n",
      "epoch 124, loss: 0.00\n",
      "epoch 125, loss: 0.00\n",
      "epoch 126, loss: 0.00\n",
      "epoch 127, loss: 0.00\n",
      "epoch 128, loss: 0.00\n",
      "epoch 129, loss: 0.00\n",
      "epoch 130, loss: 0.00\n",
      "epoch 131, loss: 0.00\n",
      "epoch 132, loss: 0.00\n",
      "epoch 133, loss: 0.00\n",
      "epoch 134, loss: 0.00\n",
      "epoch 135, loss: 0.00\n",
      "epoch 136, loss: 0.00\n",
      "epoch 137, loss: 0.00\n",
      "epoch 138, loss: 0.00\n",
      "epoch 139, loss: 0.00\n",
      "epoch 140, loss: 0.00\n",
      "epoch 141, loss: 0.00\n",
      "epoch 142, loss: 0.00\n",
      "epoch 143, loss: 0.00\n",
      "epoch 144, loss: 0.00\n",
      "epoch 145, loss: 0.00\n",
      "epoch 146, loss: 0.00\n",
      "epoch 147, loss: 0.00\n",
      "epoch 148, loss: 0.00\n",
      "epoch 149, loss: 0.00\n",
      "epoch 150, loss: 0.00\n",
      "epoch 151, loss: 0.00\n",
      "epoch 152, loss: 0.00\n",
      "epoch 153, loss: 0.00\n",
      "epoch 154, loss: 0.00\n",
      "epoch 155, loss: 0.00\n",
      "epoch 156, loss: 0.00\n",
      "epoch 157, loss: 0.00\n",
      "epoch 158, loss: 0.00\n",
      "epoch 159, loss: 0.00\n",
      "epoch 160, loss: 0.00\n",
      "epoch 161, loss: 0.00\n",
      "epoch 162, loss: 0.00\n",
      "epoch 163, loss: 0.00\n",
      "epoch 164, loss: 0.00\n",
      "epoch 165, loss: 0.00\n",
      "epoch 166, loss: 0.00\n",
      "epoch 167, loss: 0.00\n",
      "epoch 168, loss: 0.00\n",
      "epoch 169, loss: 0.00\n",
      "epoch 170, loss: 0.00\n",
      "epoch 171, loss: 0.00\n",
      "epoch 172, loss: 0.00\n",
      "epoch 173, loss: 0.00\n",
      "epoch 174, loss: 0.00\n",
      "epoch 175, loss: 0.00\n",
      "epoch 176, loss: 0.00\n",
      "epoch 177, loss: 0.00\n",
      "epoch 178, loss: 0.00\n",
      "epoch 179, loss: 0.00\n",
      "epoch 180, loss: 0.00\n",
      "epoch 181, loss: 0.00\n",
      "epoch 182, loss: 0.00\n",
      "epoch 183, loss: 0.00\n",
      "epoch 184, loss: 0.00\n",
      "epoch 185, loss: 0.00\n",
      "epoch 186, loss: 0.00\n",
      "epoch 187, loss: 0.00\n",
      "epoch 188, loss: 0.00\n",
      "epoch 189, loss: 0.00\n",
      "epoch 190, loss: 0.00\n",
      "epoch 191, loss: 0.00\n",
      "epoch 192, loss: 0.00\n",
      "epoch 193, loss: 0.00\n",
      "epoch 194, loss: 0.00\n",
      "epoch 195, loss: 0.00\n",
      "epoch 196, loss: 0.00\n",
      "epoch 197, loss: 0.00\n",
      "epoch 198, loss: 0.00\n",
      "epoch 199, loss: 0.00\n",
      "epoch 200, loss: 0.00\n",
      "epoch 201, loss: 0.00\n",
      "epoch 202, loss: 0.00\n",
      "epoch 203, loss: 0.00\n",
      "epoch 204, loss: 0.00\n",
      "epoch 205, loss: 0.00\n",
      "epoch 206, loss: 0.00\n",
      "epoch 207, loss: 0.00\n",
      "epoch 208, loss: 0.00\n",
      "epoch 209, loss: 0.00\n",
      "epoch 210, loss: 0.00\n",
      "epoch 211, loss: 0.00\n",
      "epoch 212, loss: 0.00\n",
      "epoch 213, loss: 0.00\n",
      "epoch 214, loss: 0.00\n",
      "epoch 215, loss: 0.00\n",
      "epoch 216, loss: 0.00\n",
      "epoch 217, loss: 0.00\n",
      "epoch 218, loss: 0.00\n",
      "epoch 219, loss: 0.00\n",
      "epoch 220, loss: 0.00\n",
      "epoch 221, loss: 0.00\n",
      "epoch 222, loss: 0.00\n",
      "epoch 223, loss: 0.00\n",
      "epoch 224, loss: 0.00\n",
      "epoch 225, loss: 0.00\n",
      "epoch 226, loss: 0.00\n",
      "epoch 227, loss: 0.00\n",
      "epoch 228, loss: 0.00\n",
      "epoch 229, loss: 0.00\n",
      "epoch 230, loss: 0.00\n",
      "epoch 231, loss: 0.00\n",
      "epoch 232, loss: 0.00\n",
      "epoch 233, loss: 0.00\n",
      "epoch 234, loss: 0.00\n",
      "epoch 235, loss: 0.00\n",
      "epoch 236, loss: 0.00\n",
      "epoch 237, loss: 0.00\n",
      "epoch 238, loss: 0.00\n",
      "epoch 239, loss: 0.00\n",
      "epoch 240, loss: 0.00\n",
      "epoch 241, loss: 0.00\n",
      "epoch 242, loss: 0.00\n",
      "epoch 243, loss: 0.00\n",
      "epoch 244, loss: 0.00\n",
      "epoch 245, loss: 0.00\n",
      "epoch 246, loss: 0.00\n",
      "epoch 247, loss: 0.00\n",
      "epoch 248, loss: 0.00\n",
      "epoch 249, loss: 0.00\n",
      "epoch 250, loss: 0.00\n",
      "epoch 251, loss: 0.00\n",
      "epoch 252, loss: 0.00\n",
      "epoch 253, loss: 0.00\n",
      "epoch 254, loss: 0.00\n",
      "epoch 255, loss: 0.00\n",
      "epoch 256, loss: 0.00\n",
      "epoch 257, loss: 0.00\n",
      "epoch 258, loss: 0.00\n",
      "epoch 259, loss: 0.00\n",
      "epoch 260, loss: 0.00\n",
      "epoch 261, loss: 0.00\n",
      "epoch 262, loss: 0.00\n",
      "epoch 263, loss: 0.00\n",
      "epoch 264, loss: 0.00\n",
      "epoch 265, loss: 0.00\n",
      "epoch 266, loss: 0.00\n",
      "epoch 267, loss: 0.00\n",
      "epoch 268, loss: 0.00\n",
      "epoch 269, loss: 0.00\n",
      "epoch 270, loss: 0.00\n",
      "epoch 271, loss: 0.00\n",
      "epoch 272, loss: 0.00\n",
      "epoch 273, loss: 0.00\n",
      "epoch 274, loss: 0.00\n",
      "epoch 275, loss: 0.00\n",
      "epoch 276, loss: 0.00\n",
      "epoch 277, loss: 0.00\n",
      "epoch 278, loss: 0.00\n",
      "epoch 279, loss: 0.00\n",
      "epoch 280, loss: 0.00\n",
      "epoch 281, loss: 0.00\n",
      "epoch 282, loss: 0.00\n",
      "epoch 283, loss: 0.00\n",
      "epoch 284, loss: 0.00\n",
      "epoch 285, loss: 0.00\n",
      "epoch 286, loss: 0.00\n",
      "epoch 287, loss: 0.00\n",
      "epoch 288, loss: 0.00\n",
      "epoch 289, loss: 0.00\n",
      "epoch 290, loss: 0.00\n",
      "epoch 291, loss: 0.00\n",
      "epoch 292, loss: 0.00\n",
      "epoch 293, loss: 0.00\n",
      "epoch 294, loss: 0.00\n",
      "epoch 295, loss: 0.00\n",
      "epoch 296, loss: 0.00\n",
      "epoch 297, loss: 0.00\n",
      "epoch 298, loss: 0.00\n",
      "epoch 299, loss: 0.00\n",
      "epoch 300, loss: 0.00\n",
      "epoch 301, loss: 0.00\n",
      "epoch 302, loss: 0.00\n",
      "epoch 303, loss: 0.00\n",
      "epoch 304, loss: 0.00\n",
      "epoch 305, loss: 0.00\n",
      "epoch 306, loss: 0.00\n",
      "epoch 307, loss: 0.00\n",
      "epoch 308, loss: 0.00\n",
      "epoch 309, loss: 0.00\n",
      "epoch 310, loss: 0.00\n",
      "epoch 311, loss: 0.00\n",
      "epoch 312, loss: 0.00\n",
      "epoch 313, loss: 0.00\n",
      "epoch 314, loss: 0.00\n",
      "epoch 315, loss: 0.00\n",
      "epoch 316, loss: 0.00\n",
      "epoch 317, loss: 0.00\n",
      "epoch 318, loss: 0.00\n",
      "epoch 319, loss: 0.00\n",
      "epoch 320, loss: 0.00\n",
      "epoch 321, loss: 0.00\n",
      "epoch 322, loss: 0.00\n",
      "epoch 323, loss: 0.00\n",
      "epoch 324, loss: 0.00\n",
      "epoch 325, loss: 0.00\n",
      "epoch 326, loss: 0.00\n",
      "epoch 327, loss: 0.00\n",
      "epoch 328, loss: 0.00\n",
      "epoch 329, loss: 0.00\n",
      "epoch 330, loss: 0.00\n",
      "epoch 331, loss: 0.00\n",
      "epoch 332, loss: 0.00\n",
      "epoch 333, loss: 0.00\n",
      "epoch 334, loss: 0.00\n",
      "epoch 335, loss: 0.00\n",
      "epoch 336, loss: 0.00\n",
      "epoch 337, loss: 0.00\n",
      "epoch 338, loss: 0.00\n",
      "epoch 339, loss: 0.00\n",
      "epoch 340, loss: 0.00\n",
      "epoch 341, loss: 0.00\n",
      "epoch 342, loss: 0.00\n",
      "epoch 343, loss: 0.00\n",
      "epoch 344, loss: 0.00\n",
      "epoch 345, loss: 0.00\n",
      "epoch 346, loss: 0.00\n",
      "epoch 347, loss: 0.00\n",
      "epoch 348, loss: 0.00\n",
      "epoch 349, loss: 0.00\n",
      "epoch 350, loss: 0.00\n",
      "epoch 351, loss: 0.00\n",
      "epoch 352, loss: 0.00\n",
      "epoch 353, loss: 0.00\n",
      "epoch 354, loss: 0.00\n",
      "epoch 355, loss: 0.00\n",
      "epoch 356, loss: 0.00\n",
      "epoch 357, loss: 0.00\n",
      "epoch 358, loss: 0.00\n",
      "epoch 359, loss: 0.00\n",
      "epoch 360, loss: 0.00\n",
      "epoch 361, loss: 0.00\n",
      "epoch 362, loss: 0.00\n",
      "epoch 363, loss: 0.00\n",
      "epoch 364, loss: 0.00\n",
      "epoch 365, loss: 0.00\n",
      "epoch 366, loss: 0.00\n",
      "epoch 367, loss: 0.00\n",
      "epoch 368, loss: 0.00\n",
      "epoch 369, loss: 0.00\n",
      "epoch 370, loss: 0.00\n",
      "epoch 371, loss: 0.00\n",
      "epoch 372, loss: 0.00\n",
      "epoch 373, loss: 0.00\n",
      "epoch 374, loss: 0.00\n",
      "epoch 375, loss: 0.00\n",
      "epoch 376, loss: 0.00\n",
      "epoch 377, loss: 0.00\n",
      "epoch 378, loss: 0.00\n",
      "epoch 379, loss: 0.00\n",
      "epoch 380, loss: 0.00\n",
      "epoch 381, loss: 0.00\n",
      "epoch 382, loss: 0.00\n",
      "epoch 383, loss: 0.00\n",
      "epoch 384, loss: 0.00\n",
      "epoch 385, loss: 0.00\n",
      "epoch 386, loss: 0.00\n",
      "epoch 387, loss: 0.00\n",
      "epoch 388, loss: 0.00\n",
      "epoch 389, loss: 0.00\n",
      "epoch 390, loss: 0.00\n",
      "epoch 391, loss: 0.00\n",
      "epoch 392, loss: 0.00\n",
      "epoch 393, loss: 0.00\n",
      "epoch 394, loss: 0.00\n",
      "epoch 395, loss: 0.00\n",
      "epoch 396, loss: 0.00\n",
      "epoch 397, loss: 0.00\n",
      "epoch 398, loss: 0.00\n",
      "epoch 399, loss: 0.00\n",
      "epoch 400, loss: 0.00\n",
      "epoch 401, loss: 0.00\n",
      "epoch 402, loss: 0.00\n",
      "epoch 403, loss: 0.00\n",
      "epoch 404, loss: 0.00\n",
      "epoch 405, loss: 0.00\n",
      "epoch 406, loss: 0.00\n",
      "epoch 407, loss: 0.00\n",
      "epoch 408, loss: 0.00\n",
      "epoch 409, loss: 0.00\n",
      "epoch 410, loss: 0.00\n",
      "epoch 411, loss: 0.00\n",
      "epoch 412, loss: 0.00\n",
      "epoch 413, loss: 0.00\n",
      "epoch 414, loss: 0.00\n",
      "epoch 415, loss: 0.00\n",
      "epoch 416, loss: 0.00\n",
      "epoch 417, loss: 0.00\n",
      "epoch 418, loss: 0.00\n",
      "epoch 419, loss: 0.00\n",
      "epoch 420, loss: 0.00\n",
      "epoch 421, loss: 0.00\n",
      "epoch 422, loss: 0.00\n",
      "epoch 423, loss: 0.00\n",
      "epoch 424, loss: 0.00\n",
      "epoch 425, loss: 0.00\n",
      "epoch 426, loss: 0.00\n",
      "epoch 427, loss: 0.00\n",
      "epoch 428, loss: 0.00\n",
      "epoch 429, loss: 0.00\n",
      "epoch 430, loss: 0.00\n",
      "epoch 431, loss: 0.00\n",
      "epoch 432, loss: 0.00\n",
      "epoch 433, loss: 0.00\n",
      "epoch 434, loss: 0.00\n",
      "epoch 435, loss: 0.00\n",
      "epoch 436, loss: 0.00\n",
      "epoch 437, loss: 0.00\n",
      "epoch 438, loss: 0.00\n",
      "epoch 439, loss: 0.00\n",
      "epoch 440, loss: 0.00\n",
      "epoch 441, loss: 0.00\n",
      "epoch 442, loss: 0.00\n",
      "epoch 443, loss: 0.00\n",
      "epoch 444, loss: 0.00\n",
      "epoch 445, loss: 0.00\n",
      "epoch 446, loss: 0.00\n",
      "epoch 447, loss: 0.00\n",
      "epoch 448, loss: 0.00\n",
      "epoch 449, loss: 0.00\n",
      "epoch 450, loss: 0.00\n",
      "epoch 451, loss: 0.00\n",
      "epoch 452, loss: 0.00\n",
      "epoch 453, loss: 0.00\n",
      "epoch 454, loss: 0.00\n",
      "epoch 455, loss: 0.00\n",
      "epoch 456, loss: 0.00\n",
      "epoch 457, loss: 0.00\n",
      "epoch 458, loss: 0.00\n",
      "epoch 459, loss: 0.00\n",
      "epoch 460, loss: 0.00\n",
      "epoch 461, loss: 0.00\n",
      "epoch 462, loss: 0.00\n",
      "epoch 463, loss: 0.00\n",
      "epoch 464, loss: 0.00\n",
      "epoch 465, loss: 0.00\n",
      "epoch 466, loss: 0.00\n",
      "epoch 467, loss: 0.00\n",
      "epoch 468, loss: 0.00\n",
      "epoch 469, loss: 0.00\n",
      "epoch 470, loss: 0.00\n",
      "epoch 471, loss: 0.00\n",
      "epoch 472, loss: 0.00\n",
      "epoch 473, loss: 0.00\n",
      "epoch 474, loss: 0.00\n",
      "epoch 475, loss: 0.00\n",
      "epoch 476, loss: 0.00\n",
      "epoch 477, loss: 0.00\n",
      "epoch 478, loss: 0.00\n",
      "epoch 479, loss: 0.00\n",
      "epoch 480, loss: 0.00\n",
      "epoch 481, loss: 0.00\n",
      "epoch 482, loss: 0.00\n",
      "epoch 483, loss: 0.00\n",
      "epoch 484, loss: 0.00\n",
      "epoch 485, loss: 0.00\n",
      "epoch 486, loss: 0.00\n",
      "epoch 487, loss: 0.00\n",
      "epoch 488, loss: 0.00\n",
      "epoch 489, loss: 0.00\n",
      "epoch 490, loss: 0.00\n",
      "epoch 491, loss: 0.00\n",
      "epoch 492, loss: 0.00\n",
      "epoch 493, loss: 0.00\n",
      "epoch 494, loss: 0.00\n",
      "epoch 495, loss: 0.00\n",
      "epoch 496, loss: 0.00\n",
      "epoch 497, loss: 0.00\n",
      "epoch 498, loss: 0.00\n",
      "epoch 499, loss: 0.00\n",
      "epoch 500, loss: 0.00\n",
      "epoch 501, loss: 0.00\n",
      "epoch 502, loss: 0.00\n",
      "epoch 503, loss: 0.00\n",
      "epoch 504, loss: 0.00\n",
      "epoch 505, loss: 0.00\n",
      "epoch 506, loss: 0.00\n",
      "epoch 507, loss: 0.00\n",
      "epoch 508, loss: 0.00\n",
      "epoch 509, loss: 0.00\n",
      "epoch 510, loss: 0.00\n",
      "epoch 511, loss: 0.00\n",
      "epoch 512, loss: 0.00\n",
      "epoch 513, loss: 0.00\n",
      "epoch 514, loss: 0.00\n",
      "epoch 515, loss: 0.00\n",
      "epoch 516, loss: 0.00\n",
      "epoch 517, loss: 0.00\n",
      "epoch 518, loss: 0.00\n",
      "epoch 519, loss: 0.00\n",
      "epoch 520, loss: 0.00\n",
      "epoch 521, loss: 0.00\n",
      "epoch 522, loss: 0.00\n",
      "epoch 523, loss: 0.00\n",
      "epoch 524, loss: 0.00\n",
      "epoch 525, loss: 0.00\n",
      "epoch 526, loss: 0.00\n",
      "epoch 527, loss: 0.00\n",
      "epoch 528, loss: 0.00\n",
      "epoch 529, loss: 0.00\n",
      "epoch 530, loss: 0.00\n",
      "epoch 531, loss: 0.00\n",
      "epoch 532, loss: 0.00\n",
      "epoch 533, loss: 0.00\n",
      "epoch 534, loss: 0.00\n",
      "epoch 535, loss: 0.00\n",
      "epoch 536, loss: 0.00\n",
      "epoch 537, loss: 0.00\n",
      "epoch 538, loss: 0.00\n",
      "epoch 539, loss: 0.00\n",
      "epoch 540, loss: 0.00\n",
      "epoch 541, loss: 0.00\n",
      "epoch 542, loss: 0.00\n",
      "epoch 543, loss: 0.00\n",
      "epoch 544, loss: 0.00\n",
      "epoch 545, loss: 0.00\n",
      "epoch 546, loss: 0.00\n",
      "epoch 547, loss: 0.00\n",
      "epoch 548, loss: 0.00\n",
      "epoch 549, loss: 0.00\n",
      "epoch 550, loss: 0.00\n",
      "epoch 551, loss: 0.00\n",
      "epoch 552, loss: 0.00\n",
      "epoch 553, loss: 0.00\n",
      "epoch 554, loss: 0.00\n",
      "epoch 555, loss: 0.00\n",
      "epoch 556, loss: 0.00\n",
      "epoch 557, loss: 0.00\n",
      "epoch 558, loss: 0.00\n",
      "epoch 559, loss: 0.00\n",
      "epoch 560, loss: 0.00\n",
      "epoch 561, loss: 0.00\n",
      "epoch 562, loss: 0.00\n",
      "epoch 563, loss: 0.00\n",
      "epoch 564, loss: 0.00\n",
      "epoch 565, loss: 0.00\n",
      "epoch 566, loss: 0.00\n",
      "epoch 567, loss: 0.00\n",
      "epoch 568, loss: 0.00\n",
      "epoch 569, loss: 0.00\n",
      "epoch 570, loss: 0.00\n",
      "epoch 571, loss: 0.00\n",
      "epoch 572, loss: 0.00\n",
      "epoch 573, loss: 0.00\n",
      "epoch 574, loss: 0.00\n",
      "epoch 575, loss: 0.00\n",
      "epoch 576, loss: 0.00\n",
      "epoch 577, loss: 0.00\n",
      "epoch 578, loss: 0.00\n",
      "epoch 579, loss: 0.00\n",
      "epoch 580, loss: 0.00\n",
      "epoch 581, loss: 0.00\n",
      "epoch 582, loss: 0.00\n",
      "epoch 583, loss: 0.00\n",
      "epoch 584, loss: 0.00\n",
      "epoch 585, loss: 0.00\n",
      "epoch 586, loss: 0.00\n",
      "epoch 587, loss: 0.00\n",
      "epoch 588, loss: 0.00\n",
      "epoch 589, loss: 0.00\n",
      "epoch 590, loss: 0.00\n",
      "epoch 591, loss: 0.00\n",
      "epoch 592, loss: 0.00\n",
      "epoch 593, loss: 0.00\n",
      "epoch 594, loss: 0.00\n",
      "epoch 595, loss: 0.00\n",
      "epoch 596, loss: 0.00\n",
      "epoch 597, loss: 0.00\n",
      "epoch 598, loss: 0.00\n",
      "epoch 599, loss: 0.00\n",
      "epoch 600, loss: 0.00\n",
      "epoch 601, loss: 0.00\n",
      "epoch 602, loss: 0.00\n",
      "epoch 603, loss: 0.00\n",
      "epoch 604, loss: 0.00\n",
      "epoch 605, loss: 0.00\n",
      "epoch 606, loss: 0.00\n",
      "epoch 607, loss: 0.00\n",
      "epoch 608, loss: 0.00\n",
      "epoch 609, loss: 0.00\n",
      "epoch 610, loss: 0.00\n",
      "epoch 611, loss: 0.00\n",
      "epoch 612, loss: 0.00\n",
      "epoch 613, loss: 0.00\n",
      "epoch 614, loss: 0.00\n",
      "epoch 615, loss: 0.00\n",
      "epoch 616, loss: 0.00\n",
      "epoch 617, loss: 0.00\n",
      "epoch 618, loss: 0.00\n",
      "epoch 619, loss: 0.00\n",
      "epoch 620, loss: 0.00\n",
      "epoch 621, loss: 0.00\n",
      "epoch 622, loss: 0.00\n",
      "epoch 623, loss: 0.00\n",
      "epoch 624, loss: 0.00\n",
      "epoch 625, loss: 0.00\n",
      "epoch 626, loss: 0.00\n",
      "epoch 627, loss: 0.00\n",
      "epoch 628, loss: 0.00\n",
      "epoch 629, loss: 0.00\n",
      "epoch 630, loss: 0.00\n",
      "epoch 631, loss: 0.00\n",
      "epoch 632, loss: 0.00\n",
      "epoch 633, loss: 0.00\n",
      "epoch 634, loss: 0.00\n",
      "epoch 635, loss: 0.00\n",
      "epoch 636, loss: 0.00\n",
      "epoch 637, loss: 0.00\n",
      "epoch 638, loss: 0.00\n",
      "epoch 639, loss: 0.00\n",
      "epoch 640, loss: 0.00\n",
      "epoch 641, loss: 0.00\n",
      "epoch 642, loss: 0.00\n",
      "epoch 643, loss: 0.00\n",
      "epoch 644, loss: 0.00\n",
      "epoch 645, loss: 0.00\n",
      "epoch 646, loss: 0.00\n",
      "epoch 647, loss: 0.00\n",
      "epoch 648, loss: 0.00\n",
      "epoch 649, loss: 0.00\n",
      "epoch 650, loss: 0.00\n",
      "epoch 651, loss: 0.00\n",
      "epoch 652, loss: 0.00\n",
      "epoch 653, loss: 0.00\n",
      "epoch 654, loss: 0.00\n",
      "epoch 655, loss: 0.00\n",
      "epoch 656, loss: 0.00\n",
      "epoch 657, loss: 0.00\n",
      "epoch 658, loss: 0.00\n",
      "epoch 659, loss: 0.00\n",
      "epoch 660, loss: 0.00\n",
      "epoch 661, loss: 0.00\n",
      "epoch 662, loss: 0.00\n",
      "epoch 663, loss: 0.00\n",
      "epoch 664, loss: 0.00\n",
      "epoch 665, loss: 0.00\n",
      "epoch 666, loss: 0.00\n",
      "epoch 667, loss: 0.00\n",
      "epoch 668, loss: 0.00\n",
      "epoch 669, loss: 0.00\n",
      "epoch 670, loss: 0.00\n",
      "epoch 671, loss: 0.00\n",
      "epoch 672, loss: 0.00\n",
      "epoch 673, loss: 0.00\n",
      "epoch 674, loss: 0.00\n",
      "epoch 675, loss: 0.00\n",
      "epoch 676, loss: 0.00\n",
      "epoch 677, loss: 0.00\n",
      "epoch 678, loss: 0.00\n",
      "epoch 679, loss: 0.00\n",
      "epoch 680, loss: 0.00\n",
      "epoch 681, loss: 0.00\n",
      "epoch 682, loss: 0.00\n",
      "epoch 683, loss: 0.00\n",
      "epoch 684, loss: 0.00\n",
      "epoch 685, loss: 0.00\n",
      "epoch 686, loss: 0.00\n",
      "epoch 687, loss: 0.00\n",
      "epoch 688, loss: 0.00\n",
      "epoch 689, loss: 0.00\n",
      "epoch 690, loss: 0.00\n",
      "epoch 691, loss: 0.00\n",
      "epoch 692, loss: 0.00\n",
      "epoch 693, loss: 0.00\n",
      "epoch 694, loss: 0.00\n",
      "epoch 695, loss: 0.00\n",
      "epoch 696, loss: 0.00\n",
      "epoch 697, loss: 0.00\n",
      "epoch 698, loss: 0.00\n",
      "epoch 699, loss: 0.00\n",
      "epoch 700, loss: 0.00\n",
      "epoch 701, loss: 0.00\n",
      "epoch 702, loss: 0.00\n",
      "epoch 703, loss: 0.00\n",
      "epoch 704, loss: 0.00\n",
      "epoch 705, loss: 0.00\n",
      "epoch 706, loss: 0.00\n",
      "epoch 707, loss: 0.00\n",
      "epoch 708, loss: 0.00\n",
      "epoch 709, loss: 0.00\n",
      "epoch 710, loss: 0.00\n",
      "epoch 711, loss: 0.00\n",
      "epoch 712, loss: 0.00\n",
      "epoch 713, loss: 0.00\n",
      "epoch 714, loss: 0.00\n",
      "epoch 715, loss: 0.00\n",
      "epoch 716, loss: 0.00\n",
      "epoch 717, loss: 0.00\n",
      "epoch 718, loss: 0.00\n",
      "epoch 719, loss: 0.00\n",
      "epoch 720, loss: 0.00\n",
      "epoch 721, loss: 0.00\n",
      "epoch 722, loss: 0.00\n",
      "epoch 723, loss: 0.00\n",
      "epoch 724, loss: 0.00\n",
      "epoch 725, loss: 0.00\n",
      "epoch 726, loss: 0.00\n",
      "epoch 727, loss: 0.00\n",
      "epoch 728, loss: 0.00\n",
      "epoch 729, loss: 0.00\n",
      "epoch 730, loss: 0.00\n",
      "epoch 731, loss: 0.00\n",
      "epoch 732, loss: 0.00\n",
      "epoch 733, loss: 0.00\n",
      "epoch 734, loss: 0.00\n",
      "epoch 735, loss: 0.00\n",
      "epoch 736, loss: 0.00\n",
      "epoch 737, loss: 0.00\n",
      "epoch 738, loss: 0.00\n",
      "epoch 739, loss: 0.00\n",
      "epoch 740, loss: 0.00\n",
      "epoch 741, loss: 0.00\n",
      "epoch 742, loss: 0.00\n",
      "epoch 743, loss: 0.00\n",
      "epoch 744, loss: 0.00\n",
      "epoch 745, loss: 0.00\n",
      "epoch 746, loss: 0.00\n",
      "epoch 747, loss: 0.00\n",
      "epoch 748, loss: 0.00\n",
      "epoch 749, loss: 0.00\n",
      "epoch 750, loss: 0.00\n",
      "epoch 751, loss: 0.00\n",
      "epoch 752, loss: 0.00\n",
      "epoch 753, loss: 0.00\n",
      "epoch 754, loss: 0.00\n",
      "epoch 755, loss: 0.00\n",
      "epoch 756, loss: 0.00\n",
      "epoch 757, loss: 0.00\n",
      "epoch 758, loss: 0.00\n",
      "epoch 759, loss: 0.00\n",
      "epoch 760, loss: 0.00\n",
      "epoch 761, loss: 0.00\n",
      "epoch 762, loss: 0.00\n",
      "epoch 763, loss: 0.00\n",
      "epoch 764, loss: 0.00\n",
      "epoch 765, loss: 0.00\n",
      "epoch 766, loss: 0.00\n",
      "epoch 767, loss: 0.00\n",
      "epoch 768, loss: 0.00\n",
      "epoch 769, loss: 0.00\n",
      "epoch 770, loss: 0.00\n",
      "epoch 771, loss: 0.00\n",
      "epoch 772, loss: 0.00\n",
      "epoch 773, loss: 0.00\n",
      "epoch 774, loss: 0.00\n",
      "epoch 775, loss: 0.00\n",
      "epoch 776, loss: 0.00\n",
      "epoch 777, loss: 0.00\n",
      "epoch 778, loss: 0.00\n",
      "epoch 779, loss: 0.00\n",
      "epoch 780, loss: 0.00\n",
      "epoch 781, loss: 0.00\n",
      "epoch 782, loss: 0.00\n",
      "epoch 783, loss: 0.00\n",
      "epoch 784, loss: 0.00\n",
      "epoch 785, loss: 0.00\n",
      "epoch 786, loss: 0.00\n",
      "epoch 787, loss: 0.00\n",
      "epoch 788, loss: 0.00\n",
      "epoch 789, loss: 0.00\n",
      "epoch 790, loss: 0.00\n",
      "epoch 791, loss: 0.00\n",
      "epoch 792, loss: 0.00\n",
      "epoch 793, loss: 0.00\n",
      "epoch 794, loss: 0.00\n",
      "epoch 795, loss: 0.00\n",
      "epoch 796, loss: 0.00\n",
      "epoch 797, loss: 0.00\n",
      "epoch 798, loss: 0.00\n",
      "epoch 799, loss: 0.00\n",
      "epoch 800, loss: 0.00\n",
      "epoch 801, loss: 0.00\n",
      "epoch 802, loss: 0.00\n",
      "epoch 803, loss: 0.00\n",
      "epoch 804, loss: 0.00\n",
      "epoch 805, loss: 0.00\n",
      "epoch 806, loss: 0.00\n",
      "epoch 807, loss: 0.00\n",
      "epoch 808, loss: 0.00\n",
      "epoch 809, loss: 0.00\n",
      "epoch 810, loss: 0.00\n",
      "epoch 811, loss: 0.00\n",
      "epoch 812, loss: 0.00\n",
      "epoch 813, loss: 0.00\n",
      "epoch 814, loss: 0.00\n",
      "epoch 815, loss: 0.00\n",
      "epoch 816, loss: 0.00\n",
      "epoch 817, loss: 0.00\n",
      "epoch 818, loss: 0.00\n",
      "epoch 819, loss: 0.00\n",
      "epoch 820, loss: 0.00\n",
      "epoch 821, loss: 0.00\n",
      "epoch 822, loss: 0.00\n",
      "epoch 823, loss: 0.00\n",
      "epoch 824, loss: 0.00\n",
      "epoch 825, loss: 0.00\n",
      "epoch 826, loss: 0.00\n",
      "epoch 827, loss: 0.00\n",
      "epoch 828, loss: 0.00\n",
      "epoch 829, loss: 0.00\n",
      "epoch 830, loss: 0.00\n",
      "epoch 831, loss: 0.00\n",
      "epoch 832, loss: 0.00\n",
      "epoch 833, loss: 0.00\n",
      "epoch 834, loss: 0.00\n",
      "epoch 835, loss: 0.00\n",
      "epoch 836, loss: 0.00\n",
      "epoch 837, loss: 0.00\n",
      "epoch 838, loss: 0.00\n",
      "epoch 839, loss: 0.00\n",
      "epoch 840, loss: 0.00\n",
      "epoch 841, loss: 0.00\n",
      "epoch 842, loss: 0.00\n",
      "epoch 843, loss: 0.00\n",
      "epoch 844, loss: 0.00\n",
      "epoch 845, loss: 0.00\n",
      "epoch 846, loss: 0.00\n",
      "epoch 847, loss: 0.00\n",
      "epoch 848, loss: 0.00\n",
      "epoch 849, loss: 0.00\n",
      "epoch 850, loss: 0.00\n",
      "epoch 851, loss: 0.00\n",
      "epoch 852, loss: 0.00\n",
      "epoch 853, loss: 0.00\n",
      "epoch 854, loss: 0.00\n",
      "epoch 855, loss: 0.00\n",
      "epoch 856, loss: 0.00\n",
      "epoch 857, loss: 0.00\n",
      "epoch 858, loss: 0.00\n",
      "epoch 859, loss: 0.00\n",
      "epoch 860, loss: 0.00\n",
      "epoch 861, loss: 0.00\n",
      "epoch 862, loss: 0.00\n",
      "epoch 863, loss: 0.00\n",
      "epoch 864, loss: 0.00\n",
      "epoch 865, loss: 0.00\n",
      "epoch 866, loss: 0.00\n",
      "epoch 867, loss: 0.00\n",
      "epoch 868, loss: 0.00\n",
      "epoch 869, loss: 0.00\n",
      "epoch 870, loss: 0.00\n",
      "epoch 871, loss: 0.00\n",
      "epoch 872, loss: 0.00\n",
      "epoch 873, loss: 0.00\n",
      "epoch 874, loss: 0.00\n",
      "epoch 875, loss: 0.00\n",
      "epoch 876, loss: 0.00\n",
      "epoch 877, loss: 0.00\n",
      "epoch 878, loss: 0.00\n",
      "epoch 879, loss: 0.00\n",
      "epoch 880, loss: 0.00\n",
      "epoch 881, loss: 0.00\n",
      "epoch 882, loss: 0.00\n",
      "epoch 883, loss: 0.00\n",
      "epoch 884, loss: 0.00\n",
      "epoch 885, loss: 0.00\n",
      "epoch 886, loss: 0.00\n",
      "epoch 887, loss: 0.00\n",
      "epoch 888, loss: 0.00\n",
      "epoch 889, loss: 0.00\n",
      "epoch 890, loss: 0.00\n",
      "epoch 891, loss: 0.00\n",
      "epoch 892, loss: 0.00\n",
      "epoch 893, loss: 0.00\n",
      "epoch 894, loss: 0.00\n",
      "epoch 895, loss: 0.00\n",
      "epoch 896, loss: 0.00\n",
      "epoch 897, loss: 0.00\n",
      "epoch 898, loss: 0.00\n",
      "epoch 899, loss: 0.00\n",
      "epoch 900, loss: 0.00\n",
      "epoch 901, loss: 0.00\n",
      "epoch 902, loss: 0.00\n",
      "epoch 903, loss: 0.00\n",
      "epoch 904, loss: 0.00\n",
      "epoch 905, loss: 0.00\n",
      "epoch 906, loss: 0.00\n",
      "epoch 907, loss: 0.00\n",
      "epoch 908, loss: 0.00\n",
      "epoch 909, loss: 0.00\n",
      "epoch 910, loss: 0.00\n",
      "epoch 911, loss: 0.00\n",
      "epoch 912, loss: 0.00\n",
      "epoch 913, loss: 0.00\n",
      "epoch 914, loss: 0.00\n",
      "epoch 915, loss: 0.00\n",
      "epoch 916, loss: 0.00\n",
      "epoch 917, loss: 0.00\n",
      "epoch 918, loss: 0.00\n",
      "epoch 919, loss: 0.00\n",
      "epoch 920, loss: 0.00\n",
      "epoch 921, loss: 0.00\n",
      "epoch 922, loss: 0.00\n",
      "epoch 923, loss: 0.00\n",
      "epoch 924, loss: 0.00\n",
      "epoch 925, loss: 0.00\n",
      "epoch 926, loss: 0.00\n",
      "epoch 927, loss: 0.00\n",
      "epoch 928, loss: 0.00\n",
      "epoch 929, loss: 0.00\n",
      "epoch 930, loss: 0.00\n",
      "epoch 931, loss: 0.00\n",
      "epoch 932, loss: 0.00\n",
      "epoch 933, loss: 0.00\n",
      "epoch 934, loss: 0.00\n",
      "epoch 935, loss: 0.00\n",
      "epoch 936, loss: 0.00\n",
      "epoch 937, loss: 0.00\n",
      "epoch 938, loss: 0.00\n",
      "epoch 939, loss: 0.00\n",
      "epoch 940, loss: 0.00\n",
      "epoch 941, loss: 0.00\n",
      "epoch 942, loss: 0.00\n",
      "epoch 943, loss: 0.00\n",
      "epoch 944, loss: 0.00\n",
      "epoch 945, loss: 0.00\n",
      "epoch 946, loss: 0.00\n",
      "epoch 947, loss: 0.00\n",
      "epoch 948, loss: 0.00\n",
      "epoch 949, loss: 0.00\n",
      "epoch 950, loss: 0.00\n",
      "epoch 951, loss: 0.00\n",
      "epoch 952, loss: 0.00\n",
      "epoch 953, loss: 0.00\n",
      "epoch 954, loss: 0.00\n",
      "epoch 955, loss: 0.00\n",
      "epoch 956, loss: 0.00\n",
      "epoch 957, loss: 0.00\n",
      "epoch 958, loss: 0.00\n",
      "epoch 959, loss: 0.00\n",
      "epoch 960, loss: 0.00\n",
      "epoch 961, loss: 0.00\n",
      "epoch 962, loss: 0.00\n",
      "epoch 963, loss: 0.00\n",
      "epoch 964, loss: 0.00\n",
      "epoch 965, loss: 0.00\n",
      "epoch 966, loss: 0.00\n",
      "epoch 967, loss: 0.00\n",
      "epoch 968, loss: 0.00\n",
      "epoch 969, loss: 0.00\n",
      "epoch 970, loss: 0.00\n",
      "epoch 971, loss: 0.00\n",
      "epoch 972, loss: 0.00\n",
      "epoch 973, loss: 0.00\n",
      "epoch 974, loss: 0.00\n",
      "epoch 975, loss: 0.00\n",
      "epoch 976, loss: 0.00\n",
      "epoch 977, loss: 0.00\n",
      "epoch 978, loss: 0.00\n",
      "epoch 979, loss: 0.00\n",
      "epoch 980, loss: 0.00\n",
      "epoch 981, loss: 0.00\n",
      "epoch 982, loss: 0.00\n",
      "epoch 983, loss: 0.00\n",
      "epoch 984, loss: 0.00\n",
      "epoch 985, loss: 0.00\n",
      "epoch 986, loss: 0.00\n",
      "epoch 987, loss: 0.00\n",
      "epoch 988, loss: 0.00\n",
      "epoch 989, loss: 0.00\n",
      "epoch 990, loss: 0.00\n",
      "epoch 991, loss: 0.00\n",
      "epoch 992, loss: 0.00\n",
      "epoch 993, loss: 0.00\n",
      "epoch 994, loss: 0.00\n",
      "epoch 995, loss: 0.00\n",
      "epoch 996, loss: 0.00\n",
      "epoch 997, loss: 0.00\n",
      "epoch 998, loss: 0.00\n",
      "epoch 999, loss: 0.00\n",
      "accuarcy= 0.9649\n"
     ]
    }
   ],
   "source": [
    "# 1)model\n",
    "class logisticregression(nn.Module):\n",
    "    def __init__(self,n_input_features) -> None:\n",
    "        super(logisticregression,self).__init__()\n",
    "        self.linear=nn.Linear(n_input_features,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y_pred=torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "model=logisticregression(n_features)\n",
    "\n",
    "# 2)loss optimizer\n",
    "criterion=nn.BCELoss()\n",
    "optimizer=torch.optim.Rprop(model.parameters(),lr=0.01)\n",
    "# 3)\n",
    "num_epochs=100\n",
    "for epoch in range(num_epoch):\n",
    "    # forward pass and loss\n",
    "    y_pred=model(X_train)\n",
    "    loss=criterion(y_pred,y_train)\n",
    "    # bcakward  \n",
    "    loss.backward()\n",
    "\n",
    "    # update \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f'epoch {epoch}, loss: {loss.item():.2f}')\n",
    "\n",
    "\n",
    "# evaluate use testset\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred=model(X_test)\n",
    "    y_pred_cls=y_pred.round()\n",
    "    # .eq() function to compare\n",
    "    acc = y_pred_cls.eq(y_test).sum()/y_test.shape[0]\n",
    "    print(f'accuarcy= {acc:.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "603234e39370f65517e90014d0e46b3a5857f994191af0955af6a9d6c28bcad1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
